{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b7fb3bd",
   "metadata": {},
   "source": [
    "# BERT for grammar / spell check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "federal-porcelain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "from transformers import BertTokenizer, BertForMaskedLM, BertForSequenceClassification\n",
    "import torch\n",
    "from torch.nn import Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "legitimate-sheet",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "BERT_MODEL = 'bert-base-cased'  # using a cased tokenizer because case may matter in grammar / spelling\n",
    "\n",
    "# load up a tokenizer and BERT with MLM head\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(BERT_MODEL)\n",
    "model = BertForMaskedLM.from_pretrained(BERT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfbde35c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=28996, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "muslim-limit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertOnlyMLMHead(\n",
       "  (predictions): BertLMPredictionHead(\n",
       "    (transform): BertPredictionHeadTransform(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (transform_act_fn): GELUActivation()\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): Linear(in_features=768, out_features=28996, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note the decoder's output size is the size of the tokenizer's vocab. It is crucial to use a matching tokenizer\n",
    "model.cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "religious-eating",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28996"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.vocab_size  # Looks good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee0cffc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "polished-memorabilia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_predictions(phrase, top_n=1):\n",
    "    # add a pad token before and after the phrase. \n",
    "    #  I find this helps as BERT often will neglect the first and last token otherwise\n",
    "    phrase = f'{bert_tokenizer.pad_token} {phrase} {bert_tokenizer.pad_token}'\n",
    "    \n",
    "    input_ids = bert_tokenizer.encode(phrase, return_tensors=\"pt\")  # get the input_ids from the tokenizer\n",
    "    \n",
    "    outputs = model(input_ids)  # run the input ids against BERT\n",
    "    \n",
    "    # Get the nth most confident predicted tokens from the MLM head\n",
    "    prediction_scores = outputs.logits\n",
    "    predicted_tokens = prediction_scores.argsort()[:,:,-top_n].reshape(-1,)\n",
    "    \n",
    "    # Get the probability for each token\n",
    "    token_probas = Softmax(dim=2)(prediction_scores.sort().values)[:,:,-top_n].reshape(-1, )\n",
    "    \n",
    "    for proba, token in zip(token_probas, predicted_tokens):\n",
    "        print(f'Token: {bert_tokenizer.decode([token])} ({token})  Probability: {proba:.4f}')\n",
    "        \n",
    "    return predicted_tokens\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "physical-penetration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: . (119)  Probability: 0.0636\n",
      "Token: \" (107)  Probability: 0.9721\n",
      "Token: Last (4254)  Probability: 0.8593\n",
      "Token: time (1159)  Probability: 0.9999\n",
      "Token: I (146)  Probability: 0.9995\n",
      "Token: went (1355)  Probability: 0.4761\n",
      "Token: here (1303)  Probability: 0.9999\n",
      "Token: , (117)  Probability: 1.0000\n",
      "Token: my (1139)  Probability: 0.9564\n",
      "Token: bill (4550)  Probability: 0.9953\n",
      "Token: was (1108)  Probability: 0.9999\n",
      "Token: too (1315)  Probability: 1.0000\n",
      "Token: high (1344)  Probability: 0.9989\n",
      "Token: . (119)  Probability: 1.0000\n",
      "Token: \" (107)  Probability: 0.9807\n",
      "Token: . (119)  Probability: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 119,  107, 4254, 1159,  146, 1355, 1303,  117, 1139, 4550, 1108, 1315,\n",
       "        1344,  119,  107,  119])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_predictions('Last time I went here, me bill was too high.', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fuzzy-radio",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: . (119)  Probability: 0.0563\n",
      "Token: \" (107)  Probability: 0.9262\n",
      "Token: My (1422)  Probability: 0.9989\n",
      "Token: wonderful (7310)  Probability: 0.9551\n",
      "Token: teacher (3218)  Probability: 0.9954\n",
      "Token: is (1110)  Probability: 0.9981\n",
      "Token: so (1177)  Probability: 0.9991\n",
      "Token: great (1632)  Probability: 0.9953\n",
      "Token: ! (106)  Probability: 1.0000\n",
      "Token: \" (107)  Probability: 0.9189\n",
      "Token: . (119)  Probability: 0.9683\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 119,  107, 1422, 7310, 3218, 1110, 1177, 1632,  106,  107,  119])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_predictions('My wonderful teacher is so great!', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "accepting-northern",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: , (117)  Probability: 0.0202\n",
      "Token: ' (112)  Probability: 0.0596\n",
      "Token: my (1139)  Probability: 0.0006\n",
      "Token: brilliant (8431)  Probability: 0.0154\n",
      "Token: instructor (10332)  Probability: 0.0009\n",
      "Token: was (1108)  Probability: 0.0014\n",
      "Token: very (1304)  Probability: 0.0004\n",
      "Token: wonderful (7310)  Probability: 0.0027\n",
      "Token: . (119)  Probability: 0.0000\n",
      "Token: ' (112)  Probability: 0.0763\n",
      "Token: ! (106)  Probability: 0.0311\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  117,   112,  1139,  8431, 10332,  1108,  1304,  7310,   119,   112,\n",
       "          106])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_predictions('My wonderful teacher is so great!', 2)  # 2nd choice  for wonderful is brilliant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "annual-jurisdiction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: the (1103)  Probability: 0.0174\n",
      "Token: . (119)  Probability: 0.0066\n",
      "Token: The (1109)  Probability: 0.0002\n",
      "Token: great (1632)  Probability: 0.0093\n",
      "Token: Teacher (14208)  Probability: 0.0007\n",
      "Token: isn (2762)  Probability: 0.0001\n",
      "Token: such (1216)  Probability: 0.0002\n",
      "Token: brilliant (8431)  Probability: 0.0005\n",
      "Token: ? (136)  Probability: 0.0000\n",
      "Token: ! (106)  Probability: 0.0039\n",
      "Token: ? (136)  Probability: 0.0004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 1103,   119,  1109,  1632, 14208,  2762,  1216,  8431,   136,   106,\n",
       "          136])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_predictions('My wonderful teacher is so great!', 3)  # 3rd choice  for wonderful is great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knowing-stuart",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "rough-organic",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lookahead prediction\n",
    "\n",
    "def look_ahead(phrase):\n",
    "    # add a mask token at the end\n",
    "    phrase = f'{phrase} {bert_tokenizer.mask_token} {bert_tokenizer.pad_token}'\n",
    "    \n",
    "    input_ids = bert_tokenizer.encode(phrase, return_tensors=\"pt\")  # get the input_ids from the tokenizer\n",
    "    \n",
    "    outputs = model(input_ids)  # run the input ids against BERT\n",
    "    \n",
    "    # Get the nth most confident predicted tokens from the MLM head\n",
    "    prediction_scores = outputs.logits\n",
    "    \n",
    "    for i in range(1, 4):\n",
    "        print(f'Top Score {i}')\n",
    "        predicted_tokens = prediction_scores.argsort()[:,:,-i].reshape(-1,)\n",
    "\n",
    "        # Get the probability for each token\n",
    "        token_probas = Softmax(dim=2)(prediction_scores.sort().values)[:,:,-i].reshape(-1, )\n",
    "\n",
    "        for proba, token in list(zip(token_probas, predicted_tokens))[input_ids.shape[1] - 3:]:\n",
    "            print(f'Token: {bert_tokenizer.decode([token])} ({token})  Probability: {proba:.4f}')\n",
    "        print()\n",
    "    return predicted_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "strong-gazette",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Score 1\n",
      "Token: time (1159)  Probability: 0.0528\n",
      "Token: ? (136)  Probability: 0.9924\n",
      "Token: . (119)  Probability: 0.9999\n",
      "\n",
      "Top Score 2\n",
      "Token: money (1948)  Probability: 0.0303\n",
      "Token: . (119)  Probability: 0.0056\n",
      "Token: ? (136)  Probability: 0.0001\n",
      "\n",
      "Top Score 3\n",
      "Token: numbers (2849)  Probability: 0.0271\n",
      "Token: ! (106)  Probability: 0.0015\n",
      "Token: ! (106)  Probability: 0.0000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1103,  117, 1284, 2866, 1412, 2849,  106,  106])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "look_ahead('Can we split the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "indirect-lobby",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Score 1\n",
      "Token: going (1280)  Probability: 0.8487\n",
      "Token: ? (136)  Probability: 0.9920\n",
      "Token: . (119)  Probability: 0.9986\n",
      "\n",
      "Top Score 2\n",
      "Token: now (1208)  Probability: 0.0605\n",
      "Token: . (119)  Probability: 0.0046\n",
      "Token: ? (136)  Probability: 0.0013\n",
      "\n",
      "Top Score 3\n",
      "Token: headed (2917)  Probability: 0.0298\n",
      "Token: ! (106)  Probability: 0.0032\n",
      "Token: ; (132)  Probability: 0.0000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 107,  117, 1231, 1128, 2917,  106,  132])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "look_ahead('Where are we')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "optical-investor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Score 1\n",
      "Token: unique (3527)  Probability: 0.0218\n",
      "Token: . (119)  Probability: 0.9514\n",
      "Token: . (119)  Probability: 0.9967\n",
      "\n",
      "Top Score 2\n",
      "Token: fun (4106)  Probability: 0.0216\n",
      "Token: ; (132)  Probability: 0.0225\n",
      "Token: ? (136)  Probability: 0.0010\n",
      "\n",
      "Top Score 3\n",
      "Token: special (1957)  Probability: 0.0206\n",
      "Token: ! (106)  Probability: 0.0186\n",
      "Token: ! (106)  Probability: 0.0008\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 107,  117, 4370, 1108, 1472, 4106, 1957,  106,  106])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "look_ahead('This class is kind of')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
