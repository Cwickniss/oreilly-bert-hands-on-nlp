{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "federal-porcelain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "from transformers import BertTokenizer, BertForMaskedLM, BertForSequenceClassification\n",
    "import torch\n",
    "from torch.nn import Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "legitimate-sheet",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "BERT_MODEL = 'bert-base-cased'  # using a cased tokenizer because case may matter in grammar / spelling\n",
    "\n",
    "# load up a tokenizer and BERT with MLM head\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(BERT_MODEL)\n",
    "model = BertForMaskedLM.from_pretrained(BERT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "greater-explosion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertOnlyMLMHead(\n",
       "  (predictions): BertLMPredictionHead(\n",
       "    (transform): BertPredictionHeadTransform(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): Linear(in_features=768, out_features=28996, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note the decoder's output size is the size of the tokenizer's vocab. It is crucial to use a matching tokenizer\n",
    "model.cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "competitive-deadline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28996"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.vocab_size  # Looks good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "polished-memorabilia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_predictions(phrase, top_n=1):\n",
    "    # add a pad token before and after the phrase. \n",
    "    #  I find this helps as BERT often will neglect the first and last token otherwise\n",
    "    phrase = f'{bert_tokenizer.pad_token} {phrase} {bert_tokenizer.pad_token}'\n",
    "    \n",
    "    input_ids = bert_tokenizer.encode(phrase, return_tensors=\"pt\")  # get the input_ids from the tokenizer\n",
    "    \n",
    "    outputs = model(input_ids, labels=input_ids)  # run the input ids against BERT with the labels set as the input ids\n",
    "    \n",
    "    # Get the nth most confident predicted tokens from the MLM head\n",
    "    prediction_scores = outputs[1]\n",
    "    predicted_tokens = prediction_scores.argsort()[:,:,-top_n].reshape(-1,)\n",
    "    \n",
    "    # Get the probability for each token\n",
    "    token_probas = Softmax(dim=2)(prediction_scores.sort().values)[:,:,-top_n].reshape(-1, )\n",
    "    \n",
    "    for proba, token in zip(token_probas, predicted_tokens):\n",
    "        print(f'Token: {bert_tokenizer.decode([token])} ({token})  Probability: {proba:.4f}')\n",
    "        \n",
    "    return predicted_tokens\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "physical-penetration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: . (119)  Probability: 0.0636\n",
      "Token: \" (107)  Probability: 0.9721\n",
      "Token: Last (4254)  Probability: 0.8593\n",
      "Token: time (1159)  Probability: 0.9999\n",
      "Token: I (146)  Probability: 0.9995\n",
      "Token: went (1355)  Probability: 0.4761\n",
      "Token: here (1303)  Probability: 0.9999\n",
      "Token: , (117)  Probability: 1.0000\n",
      "Token: my (1139)  Probability: 0.9564\n",
      "Token: bill (4550)  Probability: 0.9953\n",
      "Token: was (1108)  Probability: 0.9999\n",
      "Token: too (1315)  Probability: 1.0000\n",
      "Token: high (1344)  Probability: 0.9989\n",
      "Token: . (119)  Probability: 1.0000\n",
      "Token: \" (107)  Probability: 0.9807\n",
      "Token: . (119)  Probability: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 119,  107, 4254, 1159,  146, 1355, 1303,  117, 1139, 4550, 1108, 1315,\n",
       "        1344,  119,  107,  119])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_predictions('Last time I went here, me bill was too high.', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fuzzy-radio",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: . (119)  Probability: 0.0563\n",
      "Token: \" (107)  Probability: 0.9262\n",
      "Token: My (1422)  Probability: 0.9989\n",
      "Token: wonderful (7310)  Probability: 0.9551\n",
      "Token: teacher (3218)  Probability: 0.9954\n",
      "Token: is (1110)  Probability: 0.9981\n",
      "Token: so (1177)  Probability: 0.9991\n",
      "Token: great (1632)  Probability: 0.9953\n",
      "Token: ! (106)  Probability: 1.0000\n",
      "Token: \" (107)  Probability: 0.9189\n",
      "Token: . (119)  Probability: 0.9683\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 119,  107, 1422, 7310, 3218, 1110, 1177, 1632,  106,  107,  119])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_predictions('My wonderful teacher is so great!', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sound-kinase",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: , (117)  Probability: 0.0202\n",
      "Token: ' (112)  Probability: 0.0596\n",
      "Token: my (1139)  Probability: 0.0006\n",
      "Token: brilliant (8431)  Probability: 0.0154\n",
      "Token: instructor (10332)  Probability: 0.0009\n",
      "Token: was (1108)  Probability: 0.0014\n",
      "Token: very (1304)  Probability: 0.0004\n",
      "Token: wonderful (7310)  Probability: 0.0027\n",
      "Token: . (119)  Probability: 0.0000\n",
      "Token: ' (112)  Probability: 0.0763\n",
      "Token: ! (106)  Probability: 0.0311\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  117,   112,  1139,  8431, 10332,  1108,  1304,  7310,   119,   112,\n",
       "          106])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_predictions('My wonderful teacher is so great!', 2)  # 2nd choice  for wonderful is brilliant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "technical-forward",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: the (1103)  Probability: 0.0174\n",
      "Token: . (119)  Probability: 0.0066\n",
      "Token: The (1109)  Probability: 0.0002\n",
      "Token: great (1632)  Probability: 0.0093\n",
      "Token: Teacher (14208)  Probability: 0.0007\n",
      "Token: isn (2762)  Probability: 0.0001\n",
      "Token: such (1216)  Probability: 0.0002\n",
      "Token: brilliant (8431)  Probability: 0.0005\n",
      "Token: ? (136)  Probability: 0.0000\n",
      "Token: ! (106)  Probability: 0.0039\n",
      "Token: ? (136)  Probability: 0.0004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 1103,   119,  1109,  1632, 14208,  2762,  1216,  8431,   136,   106,\n",
       "          136])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_predictions('My wonderful teacher is so great!', 3)  # 3rd choice  for wonderful is great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suffering-announcement",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "rough-organic",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lookahead prediction\n",
    "\n",
    "def look_ahead(phrase):\n",
    "    # add a mask token at the end\n",
    "    phrase = f'{phrase} {bert_tokenizer.mask_token} {bert_tokenizer.pad_token}'\n",
    "    \n",
    "    input_ids = bert_tokenizer.encode(phrase, return_tensors=\"pt\")  # get the input_ids from the tokenizer\n",
    "    \n",
    "    outputs = model(input_ids, labels=input_ids)  # run the input ids against BERT with the labels set as the input ids\n",
    "    \n",
    "    # Get the nth most confident predicted tokens from the MLM head\n",
    "    prediction_scores = outputs[1]\n",
    "    \n",
    "    for i in range(1, 4):\n",
    "        print(f'Top Score {i}')\n",
    "        predicted_tokens = prediction_scores.argsort()[:,:,-i].reshape(-1,)\n",
    "\n",
    "        # Get the probability for each token\n",
    "        token_probas = Softmax(dim=2)(prediction_scores.sort().values)[:,:,-i].reshape(-1, )\n",
    "\n",
    "        for proba, token in list(zip(token_probas, predicted_tokens))[input_ids.shape[1] - 3:]:\n",
    "            print(f'Token: {bert_tokenizer.decode([token])} ({token})  Probability: {proba:.4f}')\n",
    "        print()\n",
    "    return predicted_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "differential-strip",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Score 1\n",
      "Token: time (1159)  Probability: 0.0528\n",
      "Token: ? (136)  Probability: 0.9924\n",
      "Token: . (119)  Probability: 0.9999\n",
      "\n",
      "Top Score 2\n",
      "Token: money (1948)  Probability: 0.0303\n",
      "Token: . (119)  Probability: 0.0056\n",
      "Token: ? (136)  Probability: 0.0001\n",
      "\n",
      "Top Score 3\n",
      "Token: numbers (2849)  Probability: 0.0271\n",
      "Token: ! (106)  Probability: 0.0015\n",
      "Token: ! (106)  Probability: 0.0000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1103,  117, 1284, 2866, 1412, 2849,  106,  106])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "look_ahead('Can we split the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "conceptual-vehicle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Score 1\n",
      "Token: going (1280)  Probability: 0.8487\n",
      "Token: ? (136)  Probability: 0.9920\n",
      "Token: . (119)  Probability: 0.9986\n",
      "\n",
      "Top Score 2\n",
      "Token: now (1208)  Probability: 0.0605\n",
      "Token: . (119)  Probability: 0.0046\n",
      "Token: ? (136)  Probability: 0.0013\n",
      "\n",
      "Top Score 3\n",
      "Token: headed (2917)  Probability: 0.0298\n",
      "Token: ! (106)  Probability: 0.0032\n",
      "Token: ; (132)  Probability: 0.0000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 107,  117, 1231, 1128, 2917,  106,  132])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "look_ahead('Where are we')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "changing-hamilton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Score 1\n",
      "Token: unique (3527)  Probability: 0.0218\n",
      "Token: . (119)  Probability: 0.9514\n",
      "Token: . (119)  Probability: 0.9967\n",
      "\n",
      "Top Score 2\n",
      "Token: fun (4106)  Probability: 0.0216\n",
      "Token: ; (132)  Probability: 0.0225\n",
      "Token: ? (136)  Probability: 0.0010\n",
      "\n",
      "Top Score 3\n",
      "Token: special (1957)  Probability: 0.0206\n",
      "Token: ! (106)  Probability: 0.0186\n",
      "Token: ! (106)  Probability: 0.0008\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 107,  117, 4370, 1108, 1472, 4106, 1957,  106,  106])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "look_ahead('This class is kind of')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-substitute",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "academic-football",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try fine-tuned model on Cola\n",
    "\n",
    "# https://nyu-mll.github.io/CoLA/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "criminal-grain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 8,551\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_source</th>\n",
       "      <th>label</th>\n",
       "      <th>label_notes</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2739</th>\n",
       "      <td>l-93</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Carmen bought a dress.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5552</th>\n",
       "      <td>b_73</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>They may grow as much as six feet high.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1574</th>\n",
       "      <td>r-67</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>If he sees a hot dog, he'll bring me one.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>bc01</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>John kisses often Mary.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5847</th>\n",
       "      <td>c_13</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I think he will eat asparagus.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4920</th>\n",
       "      <td>ks08</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>a pencil with that to write broke.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7203</th>\n",
       "      <td>sks13</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>it is the tall man coming from England that Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7320</th>\n",
       "      <td>sks13</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I said this.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2042</th>\n",
       "      <td>rhl07</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I shipped the package halfway to the Antarctic.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5046</th>\n",
       "      <td>ks08</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I do not think it unreasonable to ask for the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentence_source  label label_notes  \\\n",
       "2739            l-93      1         NaN   \n",
       "5552            b_73      1         NaN   \n",
       "1574            r-67      1         NaN   \n",
       "352             bc01      0           *   \n",
       "5847            c_13      1         NaN   \n",
       "4920            ks08      0           *   \n",
       "7203           sks13      0           *   \n",
       "7320           sks13      1         NaN   \n",
       "2042           rhl07      1         NaN   \n",
       "5046            ks08      1         NaN   \n",
       "\n",
       "                                               sentence  \n",
       "2739                             Carmen bought a dress.  \n",
       "5552            They may grow as much as six feet high.  \n",
       "1574          If he sees a hot dog, he'll bring me one.  \n",
       "352                             John kisses often Mary.  \n",
       "5847                     I think he will eat asparagus.  \n",
       "4920                 a pencil with that to write broke.  \n",
       "7203  it is the tall man coming from England that Ma...  \n",
       "7320                                       I said this.  \n",
       "2042    I shipped the package halfway to the Antarctic.  \n",
       "5046  I do not think it unreasonable to ask for the ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a pandas dataframe.\n",
    "cola_df = pd.read_csv(\"../data/cola.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of training sentences: {:,}\\n'.format(cola_df.shape[0]))\n",
    "\n",
    "# Display 10 random rows from the data.\n",
    "cola_df.sample(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "noble-novel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae3729c9d9240bf9be3a4146b06178d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3a289026e784acdb045247710c2dfe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "155d00f5c4a7432a8ea642c25475efb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b07ed8efc864d948a1cfb416ac00703",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nlp import load_dataset, Dataset\n",
    "\n",
    "\n",
    "cola_dataset = Dataset.from_pandas(cola_df.sample(1000, random_state=42))\n",
    "\n",
    "# Dataset has a built in train test split method\n",
    "cola_dataset = cola_dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "train_set = cola_dataset['train']\n",
    "test_set = cola_dataset['test']\n",
    "\n",
    "# We will pad our dataset so that our input matrices are the same length and truncate anything longer than 512 tokens\n",
    "def preprocess(data):\n",
    "    return bert_tokenizer(data['sentence'], padding=True, truncation=True)\n",
    "\n",
    "train_set = train_set.map(preprocess, batched=True, batch_size=len(train_set))\n",
    "test_set = test_set.map(preprocess, batched=True, batch_size=len(test_set))\n",
    "\n",
    "train_set.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_set.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bronze-examination",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "sequence_classification_model = BertForSequenceClassification.from_pretrained(\n",
    "    BERT_MODEL, num_labels=2,\n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# freeze all but the last 2 encoder layers in BERT to speed up training\n",
    "for param in list(sequence_classification_model.bert.parameters())[:165]:\n",
    "    param.requires_grad = False  # disable training in BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fixed-plastic",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "--load_best_model_at_end requires the save and eval strategy to match, but found\n- Evaluation strategy: IntervalStrategy.NO\n- Save strategy: IntervalStrategy.STEPS",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-ad132dc3b4ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.02\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m training_args = TrainingArguments(\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./gs/results'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mnum_train_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/training_args.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, save_strategy, save_steps, save_total_limit, save_on_each_node, no_cuda, seed, fp16, fp16_opt_level, fp16_backend, fp16_full_eval, local_rank, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, sharded_ddp, deepspeed, label_smoothing_factor, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/training_args.py\u001b[0m in \u001b[0;36m__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_best_model_at_end\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_strategy\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_strategy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    686\u001b[0m                     \u001b[0;34m\"--load_best_model_at_end requires the save and eval strategy to match, but found\\n- Evaluation \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m                     \u001b[0;34mf\"strategy: {self.evaluation_strategy}\\n- Save strategy: {self.save_strategy}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: --load_best_model_at_end requires the save and eval strategy to match, but found\n- Evaluation strategy: IntervalStrategy.NO\n- Save strategy: IntervalStrategy.STEPS"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 4\n",
    "\n",
    "warmup_steps = 50\n",
    "weight_decay = 0.02\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./gs/results',\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    warmup_steps=warmup_steps,\n",
    "    weight_decay=weight_decay,\n",
    "    logging_dir='./gs/logs',\n",
    "    load_best_model_at_end=True,\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=1,\n",
    "    logging_first_step=True\n",
    ")\n",
    "\n",
    "# Define the trainer: \n",
    "\n",
    "trainer = Trainer(\n",
    "    model=sequence_classification_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=test_set,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-vehicle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get initial metrics\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-boston",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changing-cowboy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get fine-tuned metrics\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-timothy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_grammatically_correct(text):\n",
    "    input_ids = bert_tokenizer.encode(text, return_tensors='pt')\n",
    "    return float(Softmax(dim=1)(sequence_classification_model(input_ids).logits)[0][1])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broken-trading",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_grammatically_correct('Me bar tab is too high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-spray",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_grammatically_correct('My bar tab is too high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "short-scroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_predictions('Me bar tab is to high', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-season",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
