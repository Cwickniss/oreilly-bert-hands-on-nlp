{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "federal-porcelain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "from transformers import BertTokenizer, BertForMaskedLM, BertForSequenceClassification\n",
    "import torch\n",
    "from torch.nn import Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "legitimate-sheet",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "BERT_MODEL = 'bert-base-cased'  # using a cased tokenizer because case may matter in grammar / spelling\n",
    "\n",
    "# load up a tokenizer and BERT with MLM head\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(BERT_MODEL)\n",
    "model = BertForMaskedLM.from_pretrained(BERT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "solar-tracker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertOnlyMLMHead(\n",
       "  (predictions): BertLMPredictionHead(\n",
       "    (transform): BertPredictionHeadTransform(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): Linear(in_features=768, out_features=28996, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note the decoder's output size is the size of the tokenizer's vocab. It is crucial to use a matching tokenizer\n",
    "model.cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bacterial-cloud",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28996"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.vocab_size  # Looks good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "polished-memorabilia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_predictions(phrase, top_n=1):\n",
    "    # add a pad token before and after the phrase. \n",
    "    #  I find this helps as BERT often will neglect the first and last token otherwise\n",
    "    phrase = f'{bert_tokenizer.pad_token} {phrase} {bert_tokenizer.pad_token}'\n",
    "    \n",
    "    input_ids = bert_tokenizer.encode(phrase, return_tensors=\"pt\")  # get the input_ids from the tokenizer\n",
    "    \n",
    "    outputs = model(input_ids, labels=input_ids)  # run the input ids against BERT with the labels set as the input ids\n",
    "    \n",
    "    # Get the nth most confident predicted tokens from the MLM head\n",
    "    prediction_scores = outputs[1]\n",
    "    predicted_tokens = prediction_scores.argsort()[:,:,-top_n].reshape(-1,)\n",
    "    \n",
    "    # Get the probability for each token\n",
    "    token_probas = Softmax(dim=2)(prediction_scores.sort().values)[:,:,-top_n].reshape(-1, )\n",
    "    \n",
    "    for proba, token in zip(token_probas, predicted_tokens):\n",
    "        print(f'Token: {bert_tokenizer.decode([token])} ({token})  Probability: {proba:.4f}')\n",
    "        \n",
    "    return predicted_tokens\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "physical-penetration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: . (119)  Probability: 0.0636\n",
      "Token: \" (107)  Probability: 0.9721\n",
      "Token: Last (4254)  Probability: 0.8593\n",
      "Token: time (1159)  Probability: 0.9999\n",
      "Token: I (146)  Probability: 0.9995\n",
      "Token: went (1355)  Probability: 0.4761\n",
      "Token: here (1303)  Probability: 0.9999\n",
      "Token: , (117)  Probability: 1.0000\n",
      "Token: my (1139)  Probability: 0.9564\n",
      "Token: bill (4550)  Probability: 0.9953\n",
      "Token: was (1108)  Probability: 0.9999\n",
      "Token: too (1315)  Probability: 1.0000\n",
      "Token: high (1344)  Probability: 0.9989\n",
      "Token: . (119)  Probability: 1.0000\n",
      "Token: \" (107)  Probability: 0.9807\n",
      "Token: . (119)  Probability: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 119,  107, 4254, 1159,  146, 1355, 1303,  117, 1139, 4550, 1108, 1315,\n",
       "        1344,  119,  107,  119])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_predictions('Last time I went here, me bill was too high.', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fuzzy-radio",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: . (119)  Probability: 0.0563\n",
      "Token: \" (107)  Probability: 0.9262\n",
      "Token: My (1422)  Probability: 0.9989\n",
      "Token: wonderful (7310)  Probability: 0.9551\n",
      "Token: teacher (3218)  Probability: 0.9954\n",
      "Token: is (1110)  Probability: 0.9981\n",
      "Token: so (1177)  Probability: 0.9991\n",
      "Token: great (1632)  Probability: 0.9953\n",
      "Token: ! (106)  Probability: 1.0000\n",
      "Token: \" (107)  Probability: 0.9189\n",
      "Token: . (119)  Probability: 0.9683\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 119,  107, 1422, 7310, 3218, 1110, 1177, 1632,  106,  107,  119])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_predictions('My wonderful teacher is so great!', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "hybrid-speaker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: , (117)  Probability: 0.0202\n",
      "Token: ' (112)  Probability: 0.0596\n",
      "Token: my (1139)  Probability: 0.0006\n",
      "Token: brilliant (8431)  Probability: 0.0154\n",
      "Token: instructor (10332)  Probability: 0.0009\n",
      "Token: was (1108)  Probability: 0.0014\n",
      "Token: very (1304)  Probability: 0.0004\n",
      "Token: wonderful (7310)  Probability: 0.0027\n",
      "Token: . (119)  Probability: 0.0000\n",
      "Token: ' (112)  Probability: 0.0763\n",
      "Token: ! (106)  Probability: 0.0311\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  117,   112,  1139,  8431, 10332,  1108,  1304,  7310,   119,   112,\n",
       "          106])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_predictions('My wonderful teacher is so great!', 2)  # 2nd choice  for wonderful is brilliant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "indian-allowance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: the (1103)  Probability: 0.0174\n",
      "Token: . (119)  Probability: 0.0066\n",
      "Token: The (1109)  Probability: 0.0002\n",
      "Token: great (1632)  Probability: 0.0093\n",
      "Token: Teacher (14208)  Probability: 0.0007\n",
      "Token: isn (2762)  Probability: 0.0001\n",
      "Token: such (1216)  Probability: 0.0002\n",
      "Token: brilliant (8431)  Probability: 0.0005\n",
      "Token: ? (136)  Probability: 0.0000\n",
      "Token: ! (106)  Probability: 0.0039\n",
      "Token: ? (136)  Probability: 0.0004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 1103,   119,  1109,  1632, 14208,  2762,  1216,  8431,   136,   106,\n",
       "          136])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_predictions('My wonderful teacher is so great!', 3)  # 3rd choice  for wonderful is great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-charles",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "rough-organic",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lookahead prediction\n",
    "\n",
    "def look_ahead(phrase):\n",
    "    # add a mask token at the end\n",
    "    phrase = f'{phrase} {bert_tokenizer.mask_token} {bert_tokenizer.pad_token}'\n",
    "    \n",
    "    input_ids = bert_tokenizer.encode(phrase, return_tensors=\"pt\")  # get the input_ids from the tokenizer\n",
    "    \n",
    "    outputs = model(input_ids, labels=input_ids)  # run the input ids against BERT with the labels set as the input ids\n",
    "    \n",
    "    # Get the nth most confident predicted tokens from the MLM head\n",
    "    prediction_scores = outputs[1]\n",
    "    \n",
    "    for i in range(1, 4):\n",
    "        print(f'Top Score {i}')\n",
    "        predicted_tokens = prediction_scores.argsort()[:,:,-i].reshape(-1,)\n",
    "\n",
    "        # Get the probability for each token\n",
    "        token_probas = Softmax(dim=2)(prediction_scores.sort().values)[:,:,-i].reshape(-1, )\n",
    "\n",
    "        for proba, token in list(zip(token_probas, predicted_tokens))[input_ids.shape[1] - 3:]:\n",
    "            print(f'Token: {bert_tokenizer.decode([token])} ({token})  Probability: {proba:.4f}')\n",
    "        print()\n",
    "    return predicted_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "experimental-citizenship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Score 1\n",
      "Token: time (1159)  Probability: 0.0528\n",
      "Token: ? (136)  Probability: 0.9924\n",
      "Token: . (119)  Probability: 0.9999\n",
      "\n",
      "Top Score 2\n",
      "Token: money (1948)  Probability: 0.0303\n",
      "Token: . (119)  Probability: 0.0056\n",
      "Token: ? (136)  Probability: 0.0001\n",
      "\n",
      "Top Score 3\n",
      "Token: numbers (2849)  Probability: 0.0271\n",
      "Token: ! (106)  Probability: 0.0015\n",
      "Token: ! (106)  Probability: 0.0000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1103,  117, 1284, 2866, 1412, 2849,  106,  106])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "look_ahead('Can we split the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "serious-greeting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Score 1\n",
      "Token: going (1280)  Probability: 0.8487\n",
      "Token: ? (136)  Probability: 0.9920\n",
      "Token: . (119)  Probability: 0.9986\n",
      "\n",
      "Top Score 2\n",
      "Token: now (1208)  Probability: 0.0605\n",
      "Token: . (119)  Probability: 0.0046\n",
      "Token: ? (136)  Probability: 0.0013\n",
      "\n",
      "Top Score 3\n",
      "Token: headed (2917)  Probability: 0.0298\n",
      "Token: ! (106)  Probability: 0.0032\n",
      "Token: ; (132)  Probability: 0.0000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 107,  117, 1231, 1128, 2917,  106,  132])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "look_ahead('Where are we')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "alternate-struggle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Score 1\n",
      "Token: unique (3527)  Probability: 0.0218\n",
      "Token: . (119)  Probability: 0.9514\n",
      "Token: . (119)  Probability: 0.9967\n",
      "\n",
      "Top Score 2\n",
      "Token: fun (4106)  Probability: 0.0216\n",
      "Token: ; (132)  Probability: 0.0225\n",
      "Token: ? (136)  Probability: 0.0010\n",
      "\n",
      "Top Score 3\n",
      "Token: special (1957)  Probability: 0.0206\n",
      "Token: ! (106)  Probability: 0.0186\n",
      "Token: ! (106)  Probability: 0.0008\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 107,  117, 4370, 1108, 1472, 4106, 1957,  106,  106])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "look_ahead('This class is kind of')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-verse",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "conscious-nursing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try fine-tuned model on Cola\n",
    "\n",
    "# https://nyu-mll.github.io/CoLA/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eleven-batman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 8,551\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_source</th>\n",
       "      <th>label</th>\n",
       "      <th>label_notes</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4856</th>\n",
       "      <td>ks08</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>Who do you think that has given the tickets to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2310</th>\n",
       "      <td>l-93</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>Harriet alternated folk songs and pop songs to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>gj04</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fred tracked the leak to its source.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7962</th>\n",
       "      <td>ad03</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Who's there?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2029</th>\n",
       "      <td>rhl07</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I sent the package to London.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4782</th>\n",
       "      <td>ks08</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Who do you believe invited Sara?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3167</th>\n",
       "      <td>l-93</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sharon fainted.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7685</th>\n",
       "      <td>sks13</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>John convinced the rice to be cooked by Bill.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>bc01</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Louise is unhappy, isn't she?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7130</th>\n",
       "      <td>sks13</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>This girl in the red coat will put a picture o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentence_source  label label_notes  \\\n",
       "4856            ks08      0           *   \n",
       "2310            l-93      0           *   \n",
       "29              gj04      1         NaN   \n",
       "7962            ad03      1         NaN   \n",
       "2029           rhl07      1         NaN   \n",
       "4782            ks08      1         NaN   \n",
       "3167            l-93      1         NaN   \n",
       "7685           sks13      0           *   \n",
       "325             bc01      1         NaN   \n",
       "7130           sks13      0           *   \n",
       "\n",
       "                                               sentence  \n",
       "4856  Who do you think that has given the tickets to...  \n",
       "2310  Harriet alternated folk songs and pop songs to...  \n",
       "29                 Fred tracked the leak to its source.  \n",
       "7962                                       Who's there?  \n",
       "2029                      I sent the package to London.  \n",
       "4782                   Who do you believe invited Sara?  \n",
       "3167                                    Sharon fainted.  \n",
       "7685      John convinced the rice to be cooked by Bill.  \n",
       "325                       Louise is unhappy, isn't she?  \n",
       "7130  This girl in the red coat will put a picture o...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a pandas dataframe.\n",
    "cola_df = pd.read_csv(\"../data/cola.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of training sentences: {:,}\\n'.format(cola_df.shape[0]))\n",
    "\n",
    "# Display 10 random rows from the data.\n",
    "cola_df.sample(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "modern-maine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b60b1d6763ab435db672c41b5ebc459e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f366becb9b354aa2acb456eaac4823c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fc2330e36e14210a9f881d191d783b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5b0fd73c1e7406fa19647bd33376b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nlp import load_dataset, Dataset\n",
    "\n",
    "\n",
    "cola_dataset = Dataset.from_pandas(cola_df.sample(1000, random_state=42))\n",
    "\n",
    "# Dataset has a built in train test split method\n",
    "cola_dataset = cola_dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "train_set = cola_dataset['train']\n",
    "test_set = cola_dataset['test']\n",
    "\n",
    "# We will pad our dataset so that our input matrices are the same length and truncate anything longer than 512 tokens\n",
    "def preprocess(data):\n",
    "    return bert_tokenizer(data['sentence'], padding=True, truncation=True)\n",
    "\n",
    "train_set = train_set.map(preprocess, batched=True, batch_size=len(train_set))\n",
    "test_set = test_set.map(preprocess, batched=True, batch_size=len(test_set))\n",
    "\n",
    "train_set.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_set.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bronze-examination",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "sequence_classification_model = BertForSequenceClassification.from_pretrained(\n",
    "    BERT_MODEL, num_labels=2,\n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# freeze all but the last 2 encoder layers in BERT to speed up training\n",
    "for param in list(sequence_classification_model.bert.parameters())[:165]:\n",
    "    param.requires_grad = False  # disable training in BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fixed-plastic",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 4\n",
    "\n",
    "warmup_steps = 50\n",
    "weight_decay = 0.02\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./gs/results',\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    warmup_steps=warmup_steps,\n",
    "    weight_decay=weight_decay,\n",
    "    logging_dir='./gs/logs',\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=1,\n",
    "    logging_first_step=True\n",
    ")\n",
    "\n",
    "# Define the trainer: \n",
    "\n",
    "trainer = Trainer(\n",
    "    model=sequence_classification_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=test_set,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "forward-space",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 05:37]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.9257231950759888,\n",
       " 'eval_accuracy': 0.225,\n",
       " 'eval_runtime': 10.2968,\n",
       " 'eval_samples_per_second': 19.423,\n",
       " 'eval_steps_per_second': 0.68}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get initial metrics\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "brazilian-annex",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 05:11, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.904800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.903000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.871200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.795400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.939300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.924300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.823400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.801300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.844300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.798300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.874800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.826800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.794800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.789900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.741500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.726600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.700300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.688800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.687100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.654900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.654800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.634000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.684500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.608000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.669200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.638100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.748900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.569600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.726900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.580600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.594900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.679700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.552800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.572500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.580100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.621400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.700800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.772900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.501400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.578100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.682200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.530800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.619800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.573900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.600800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.625500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.589000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.541300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.567300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.520300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.635400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.485000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.643100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.578400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.573400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.530400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.575700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.621800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.678700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.630700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.544900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.610400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.567300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.522400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.593400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.601300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.620900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.539500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.548900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.474600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.554400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.471700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.488800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.571200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.700600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.504300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.494900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.546300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.506500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.534600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.541800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.486800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.551100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.523200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.512900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.605000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.470700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.510700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.536200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.583800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.447500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.492300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.539900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.592700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.421000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.553700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.635300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.495400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.619600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.358900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=0.6203889310359955, metrics={'train_runtime': 314.013, 'train_samples_per_second': 10.191, 'train_steps_per_second': 0.318, 'total_flos': 50977766976000.0, 'train_loss': 0.6203889310359955, 'epoch': 4.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "alleged-client",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4896756708621979,\n",
       " 'eval_accuracy': 0.805,\n",
       " 'eval_runtime': 14.3165,\n",
       " 'eval_samples_per_second': 13.97,\n",
       " 'eval_steps_per_second': 0.489,\n",
       " 'epoch': 4.0}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get fine-tuned metrics\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "still-potato",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_grammatically_correct(text):\n",
    "    input_ids = bert_tokenizer.encode(text, return_tensors='pt')\n",
    "    return float(Softmax(dim=1)(sequence_classification_model(input_ids).logits)[0][1])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "artificial-texas",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49089136719703674"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_grammatically_correct('Me bar tab is too high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "complete-cloud",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6457576751708984"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_grammatically_correct('My bar tab is too high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "focal-judges",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: . (119)  Probability: 0.0747\n",
      "Token: \" (107)  Probability: 0.2778\n",
      "Token: me (1143)  Probability: 0.2538\n",
      "Token: bar (2927)  Probability: 0.8779\n",
      "Token: ta (27629)  Probability: 0.9995\n",
      "Token: ##b (1830)  Probability: 0.9972\n",
      "Token: is (1110)  Probability: 0.9868\n",
      "Token: to (1106)  Probability: 0.8474\n",
      "Token: high (1344)  Probability: 0.9143\n",
      "Token: . (119)  Probability: 0.8100\n",
      "Token: . (119)  Probability: 0.9988\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  119,   107,  1143,  2927, 27629,  1830,  1110,  1106,  1344,   119,\n",
       "          119])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_predictions('Me bar tab is to high', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-default",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
